{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "# Graphs currently unavailable in this set because of experimental Graph library hiddenlayer\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt # for plotting graphs\n",
    "from functools import cmp_to_key\n",
    "\n",
    "from sklearn import metrics\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "import hiddenlayer as hl\n",
    "import time\n",
    "import random\n",
    "\n",
    "# A History object to store metrics\n",
    "history1 = hl.History()\n",
    "# A Canvas object to draw the metrics\n",
    "canvas1 = hl.Canvas()\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train.csv file\n",
    "df = pd.read_csv('train.csv')\n",
    "df = df.reindex()\n",
    "df_test = pd.read_csv('test.csv')\n",
    "numerical_fields = [\n",
    "    'LotArea', 'OverallQual', 'YearBuilt', 'TotRmsAbvGrd', 'YrSold', 'OverallCond', 'PoolArea'\n",
    "]\n",
    "\n",
    "categorical_fields = [\n",
    "    'Neighborhood', 'SaleCondition', 'MSZoning', \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feature_columns(input_features):\n",
    "    \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "    Args:\n",
    "    input_features: The names of the numerical input features to use.\n",
    "    Returns:\n",
    "    A set of feature columns\n",
    "    \"\"\"\n",
    "    return set([tf.feature_column.numeric_column(my_feature)\n",
    "                for my_feature in input_features])\n",
    "\n",
    "#Modified one_hot encoding to replace NA's\n",
    "def get_dummies(dummy_na = False):\n",
    "    def wrapper_get_dummies(s):\n",
    "        df = pd.get_dummies(s, prefix = s.name, dummy_na = dummy_na)\n",
    "        df1 = pd.DataFrame()\n",
    "        for k in df.keys():\n",
    "            s = df[k]\n",
    "            name = s.name\n",
    "            name = name.replace('(', '')\n",
    "            name = name.replace(')', '')\n",
    "            name = name.replace(' ', '')\n",
    "            s1 = s.rename(name)\n",
    "            df1[s1.name] = s1\n",
    "        return df1\n",
    "    return wrapper_get_dummies\n",
    "\n",
    "def convert_features(df, num_fields, cat_fields, num_fields_proc = None, cat_fields_proc = None, label_name = None, train_validate_ratio = None):\n",
    "    if num_fields_proc is None:\n",
    "        num_fields_proc = lambda x: x\n",
    "    if cat_fields_proc is None:\n",
    "        cat_fields_proc = lambda x: x\n",
    "    features = pd.DataFrame()\n",
    "    for k in num_fields:\n",
    "        features[k] = num_fields_proc(df[k].copy())\n",
    "    for k in cat_fields:\n",
    "        features = features.join(cat_fields_proc(df[k].copy()))\n",
    "    if label_name is not None:\n",
    "        labels = df[label_name].copy()\n",
    "    else:\n",
    "        labels = None\n",
    "    if train_validate_ratio is None:\n",
    "        train_validate_ratio = 1\n",
    "    train_num = int(len(df) * train_validate_ratio)\n",
    "    validate_num = len(df) - train_num\n",
    "    train_features = features.head(train_num)\n",
    "    validate_features = features.tail(validate_num)\n",
    "    if labels is not None:\n",
    "        train_labels = labels.head(train_num)\n",
    "        validate_labels = labels.tail(validate_num)\n",
    "    else:\n",
    "        train_labels = None\n",
    "        validate_labels = None\n",
    "    return (train_features, train_labels, validate_features, validate_labels)\n",
    "\n",
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"Trains a linear regression model.\n",
    "\n",
    "    Args:\n",
    "      features: pandas DataFrame of features\n",
    "      targets: pandas DataFrame of targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "      Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    # features = {key:np.array(value) for key,value in dict(features).items()}                                            \n",
    "\n",
    "    #\n",
    "    # note: can convert to dict directly\n",
    "    #\n",
    "    features = dict(features)\n",
    "\n",
    "\n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "\n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels\n",
    "\n",
    "def my_input_fn_pred(features, batch_size=1, shuffle=True, num_epochs=None):\n",
    "    \"\"\"Trains a linear regression model.\n",
    "\n",
    "    Args:\n",
    "      features: pandas DataFrame of features\n",
    "      targets: pandas DataFrame of targets\n",
    "      batch_size: Size of batches to be passed to the model\n",
    "      shuffle: True or False. Whether to shuffle the data.\n",
    "      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n",
    "    Returns:\n",
    "      Tuple of (features, labels) for next data batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    # features = {key:np.array(value) for key,value in dict(features).items()}                                            \n",
    "\n",
    "    #\n",
    "    # note: can convert to dict directly\n",
    "    #\n",
    "    features = dict(features)\n",
    "\n",
    "\n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = Dataset.from_tensor_slices(features) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "\n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "\n",
    "    # Return the next batch of data.\n",
    "    features = ds.make_one_shot_iterator().get_next()\n",
    "    return features\n",
    "\n",
    "def train_dnn_regressor_model(\n",
    "    optimizer,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "    \"\"\"Trains a DNN regression model.\n",
    "\n",
    "    In addition to training, this function also prints training progress information,\n",
    "    as well as a plot of the training and validation loss over time.\n",
    "\n",
    "    Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n",
    "    training_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for training.\n",
    "    training_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for training.\n",
    "    validation_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for validation.\n",
    "    validation_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for validation.\n",
    "\n",
    "    Returns:\n",
    "    A `LinearRegressor` object trained on the training data.\n",
    "    \"\"\"\n",
    "    if validation_examples is not None and validation_targets is not None:\n",
    "        do_validation = True\n",
    "    else:\n",
    "        do_validation = False\n",
    "    \n",
    "    periods = 50\n",
    "    steps_per_period = steps / periods\n",
    "\n",
    "    # Create a linear regressor object.\n",
    "    dnn_regressor = tf.estimator.DNNRegressor(\n",
    "        feature_columns=construct_feature_columns(training_examples),\n",
    "        hidden_units=hidden_units,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "\n",
    "    # Create input functions.\n",
    "    training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                            training_targets, \n",
    "                                            batch_size=batch_size)\n",
    "    predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                                    training_targets, \n",
    "                                                    num_epochs=1, \n",
    "                                                    shuffle=False)\n",
    "    if do_validation == True:\n",
    "        predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                          validation_targets, \n",
    "                                                          num_epochs=1, \n",
    "                                                          shuffle=False)\n",
    "\n",
    "    # Train the model, but do so inside a loop so that we can periodically assess\n",
    "    # loss metrics.\n",
    "    print(\"Training model...\")\n",
    "    print(\"RMSE, RMSLE (on training data):\")\n",
    "    training_rmse = []\n",
    "    validation_rmse = []\n",
    "    training_rmsle = []\n",
    "    validation_rmsle = []\n",
    "    for period in range (0, periods):\n",
    "        # Train the model, starting from the prior state.\n",
    "        dnn_regressor.train(\n",
    "            input_fn=training_input_fn,\n",
    "            steps=steps_per_period\n",
    "        )\n",
    "\n",
    "        # Take a break and compute predictions.\n",
    "        training_predictions = dnn_regressor.predict(input_fn=predict_training_input_fn)\n",
    "        training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n",
    "        \n",
    "        # Compute training loss RMSE.\n",
    "        training_root_mean_squared_error = math.sqrt(\n",
    "            metrics.mean_squared_error(training_targets, training_predictions))\n",
    "\n",
    "        # Compute training loss RMSLE.\n",
    "        training_root_mean_squared_log_error = math.sqrt(\n",
    "            metrics.mean_squared_log_error(training_targets, training_predictions))\n",
    "        \n",
    "        if do_validation == True:\n",
    "            validation_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)\n",
    "            validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
    "\n",
    "            # Compute validation loss RMSE.\n",
    "            validation_root_mean_squared_error = math.sqrt(\n",
    "                metrics.mean_squared_error(validation_targets, validation_predictions))\n",
    "\n",
    "            # Compute validation loss RMSLE.\n",
    "            validation_root_mean_squared_log_error = math.sqrt(\n",
    "                metrics.mean_squared_log_error(validation_targets, validation_predictions))\n",
    "\n",
    "        # Occasionally print the current loss.\n",
    "        print(\"  period %02d : %0.2f, %0.4f\" % (period, training_root_mean_squared_error, training_root_mean_squared_log_error))\n",
    "        # Add the loss metrics from this period to our list.\n",
    "        training_rmse.append(training_root_mean_squared_error)\n",
    "        training_rmsle.append(training_root_mean_squared_log_error)\n",
    "        if do_validation == True:\n",
    "            validation_rmse.append(validation_root_mean_squared_error)\n",
    "            validation_rmsle.append(validation_root_mean_squared_log_error)\n",
    "        \n",
    "        # Store metrics in the history object\n",
    "        history1.log(period, rmse=training_root_mean_squared_error, rmsle=training_root_mean_squared_log_error)\n",
    "\n",
    "        # Plot the two metrics in one graph\n",
    "        canvas1.draw_plot([history1[\"RMSE\"], history1[\"RMSLE\"]])\n",
    "        time.sleep(0.1)\n",
    " \n",
    "    print(\"Model training finished.\")\n",
    "    return dnn_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = convert_features(df, \n",
    "                       numerical_fields, \n",
    "                       categorical_fields, \n",
    "                       num_fields_proc = None, \n",
    "                       cat_fields_proc = get_dummies(categorical_fields),\n",
    "                       label_name = 'SalePrice',\n",
    "                       train_validate_ratio = 0.9)\n",
    "\n",
    "training_features = ret[0]\n",
    "training_targets = ret[1]\n",
    "validation_features = ret[2]\n",
    "validation_targets = ret[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-779a7430a276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtraining_targets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mvalidation_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     validation_targets=validation_targets)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-ba9caf29e7ff>\u001b[0m in \u001b[0;36mtrain_dnn_regressor_model\u001b[0;34m(optimizer, steps, batch_size, hidden_units, training_examples, training_targets, validation_examples, validation_targets)\u001b[0m\n\u001b[1;32m    194\u001b[0m         dnn_regressor.train(\n\u001b[1;32m    195\u001b[0m             \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_period\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1179\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1213\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1214\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    669\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1149\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1222\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, 5.0)\n",
    "dnn_regressor = train_dnn_regressor_model(\n",
    "    optimizer,\n",
    "    steps=100,\n",
    "    batch_size=100,\n",
    "    #hidden_units=[100,20,44,11],\n",
    "    hidden_units=[20],\n",
    "    training_examples=training_features,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_features,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = convert_features(df_test, \n",
    "                       numerical_fields, \n",
    "                       categorical_fields, \n",
    "                       num_fields_proc = None, \n",
    "                       cat_fields_proc = get_dummies(dummy_na = True),\n",
    "                       label_name = None,\n",
    "                       train_validate_ratio = 1.0)\n",
    "\n",
    "test_features = ret[0]\n",
    "\n",
    "# fill missing values with 0.0\n",
    "for k in test_features.keys():\n",
    "    s = test_features[k]\n",
    "    na_cnt = sum(s.isna())\n",
    "    if na_cnt > 0:\n",
    "        test_features[k] = s.fillna(0.0)\n",
    "\n",
    "predict_test_input_fn = lambda: my_input_fn_pred(test_features,\n",
    "                                                 num_epochs=1, \n",
    "                                                 shuffle=False)\n",
    "\n",
    "test_predictions = dnn_regressor.predict(input_fn=predict_test_input_fn)\n",
    "df_submit = pd.DataFrame()\n",
    "df_submit['Id'] = df_test['Id']\n",
    "df_submit['SalePrice'] = np.array([item['predictions'][0] for item in test_predictions])\n",
    "\n",
    "df_submit.to_csv('./test_prediction-2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
